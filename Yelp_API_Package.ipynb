{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatumanyaDuncan/secone_one-/blob/main/Yelp_API_Package.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Calls Using the YelpAPI Package\n",
        "Thankfully, we do not need to construct our API calls manually. There is a Python package for most APIs that we can leverage. For more information on how to construct API calls manually, see the optional \"Manually Constructing API Calls \" lesson at the end of this week.\n",
        "\n",
        "For Yelp, we have the YelpAPI Package ([Documentation](https://github.com/gfairchild/yelpapi))\n",
        "\n",
        "<img src=\"https://assets.codingdojo.com/boomyeah2015/codingdojo/curriculum/content/chapter/1648664855__yelpapi-package.png\">"
      ],
      "metadata": {
        "id": "ov7webx8x9UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to Use YelpAPI:\n",
        "* To use the package, we must create an instance of the YelpAPI class.\n",
        "  * We will then use this variable to execute all of our queries.  \n",
        "\n",
        "First, be sure to open your local json file with your Yelp API credentials. You should have this saved in a secret folder in a json file. If not, please refer to the \"Saving and Using API Credentials Lesson.\" Be sure to use YOUR path. It should look similar to the path used below."
      ],
      "metadata": {
        "id": "yB3OM1hbyU_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/Users/YOUR_PATH.secret/yelp_api.json') as f:\n",
        "    login = json.load(f)\n",
        "login.keys()"
      ],
      "metadata": {
        "id": "Xf1lX5W5x-5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the YelpAPI Class\n",
        "from yelpapi import YelpAPI\n",
        "# Create an instance with your key\n",
        "yelp_api = YelpAPI(login['api-key'], timeout_s=5.0)\n",
        "yelp_api"
      ],
      "metadata": {
        "id": "YoMsT65syir2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To use the \"businesses search\" endpoint,  we will use the yelp_api.search_query method.\n",
        " *  If we inspect the docstring for the function (either run the help function on it or place your cursor inside the parenthesis for it and hit Shift+Tab), we see that it doesn't tell us very much."
      ],
      "metadata": {
        "id": "v93ndNpTylMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(yelp_api.search_query)"
      ],
      "metadata": {
        "id": "0SmulWtyypPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The only arguments/parameters that the docstring mentions are location or latitude & longitude.\n",
        "* Notice that the link to the official API documentation is linked instead.\n",
        "  * This is because this documentation only shows what the required parameters are, but it will accept ANY of the parameters that are accepted by the business_search endpoint."
      ],
      "metadata": {
        "id": "JlF3ShXiyq_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use our yelp_api variable's search_query method to perform our API call\n",
        "search_results = yelp_api.search_query(location='NY, NY',\n",
        "                                       term='Pizza')\n",
        "print(type(search_results))\n",
        "search_results.keys()"
      ],
      "metadata": {
        "id": "h7P8eUuryvau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The package returns the results in the JSON format we have been exploring. Note that the exact results may vary as the Yelp site is constantly changing."
      ],
      "metadata": {
        "id": "QEEVaR6yyz5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results['total']"
      ],
      "metadata": {
        "id": "s8arc0EByyzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biz = pd.DataFrame(search_results['businesses'])\n",
        "biz.head(2)"
      ],
      "metadata": {
        "id": "BjUv9KVCy33Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Your results may vary from what is shown, but should generally look like the output above)\n",
        "Pagination and \"offset\"\n",
        "Pagination can be understood through this brief demonstration:\n",
        "\n",
        "\n",
        "**Total Results vs. Businesses**\n",
        "\n",
        "* If we check the total number of results, we can see that we had 12,000 businesses that met our search criteria."
      ],
      "metadata": {
        "id": "ER9VxqDTy7j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## total number of matching businesses\n",
        "search_results['total']"
      ],
      "metadata": {
        "id": "I4w1fm5ry_VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* However, if we check our list of businesses, we will see that we only received data for 20 businesses."
      ],
      "metadata": {
        "id": "5hhKMimxzCIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## how many businesses in our results\n",
        "len(search_results['businesses'])"
      ],
      "metadata": {
        "id": "yi8KjzGAzDpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 12000 results, but the length is only 2.? What's going on?\n",
        "\n",
        "If you think about the results of a Google search, we usually have MANY more results that Google will display at one time. When we scroll down to the bottom of the results, we can see that Google has divided the results into several PAGES of results instead of one extremely long page.\n",
        "\n",
        "<img src=\"https://assets.codingdojo.com/boomyeah2015/codingdojo/curriculum/content/chapter/1648664842__google_results_pages.png\">  \n",
        "\n",
        "This is what Yelp Fusion is doing as well! The general term for this is \"Pagination\".\n",
        "\n",
        "\n",
        "\n",
        "The Yelp API will only return a \"page\" of 20 results at a time.\n",
        "\n",
        "* If we want to get the next page of results, we will perform another API call, but we will add an additional argument called \"offset.\"\n",
        "  * The offset is what # result to use as the FIRST result for the page.\n",
        "  * If we had 20 businesses in our first result, we would want to add an offset of 20."
      ],
      "metadata": {
        "id": "aeXJFJ6GzIET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add offset to our original api call\n",
        "search_results = yelp_api.search_query(location='NY, NY',\n",
        "                                       term='Pizza',\n",
        "                                       offset = 20)\n"
      ],
      "metadata": {
        "id": "AL6pET9IzM2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biz20 = pd.DataFrame(search_results['businesses'])\n",
        "biz20.head(2)"
      ],
      "metadata": {
        "id": "R1vk_Q4YzZ6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have different results than your original call when you make the offset call.\n",
        "\n",
        "Now you can combine the results (so far) into one dataframe using pd.concat()"
      ],
      "metadata": {
        "id": "Y4v2YIeqzcwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## concatenate the previous results and new results.\n",
        "businesses = pd.concat([biz, biz20],\n",
        "                      ignore_index=True)\n",
        "display(businesses.head(3), businesses.tail(3))"
      ],
      "metadata": {
        "id": "3T1vHQgJziek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Concepts of Efficient API Extraction](https://login.codingdojo.com/m/376/12529/88076)"
      ],
      "metadata": {
        "id": "Nzbp5c4I3Z3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Efficient API Extraction"
      ],
      "metadata": {
        "id": "rttCdacu0M_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Additional Imports\n",
        "import os, json, math, time"
      ],
      "metadata": {
        "id": "yXsAXCeT0JWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Search**\n",
        "\n",
        "To allow us to easily perform different searches in the future, we will define variables for LOCATION and TERM set for our particular search conditions. Then, when we want to use a different location or term, we can just redefine these variables. This streamlines the code and makes it more readable and reproducible."
      ],
      "metadata": {
        "id": "LpkNXyCd0glX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set our API call parameters\n",
        "LOCATION = 'NY,NY'\n",
        "TERM = 'Pizza'"
      ],
      "metadata": {
        "id": "hhefVYpc0LFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a results-in-progress JSON file, but only if it doesn't exist.\n",
        "\n",
        "This is the file where your results will be saved. Note: you must rename your JSON_FILE for different queries to prevent confusing results from other searches. We recommend you include your search terms in the filename."
      ],
      "metadata": {
        "id": "PuOoKeLY0mq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying JSON_FILE filename (can include a folder)\n",
        "# include the search terms in the filename\n",
        "JSON_FILE = \"Data/results_in_progress_NY_pizza.json\"\n",
        "JSON_FILE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Vs4Gqnj10jeK",
        "outputId": "78971f45-add1-4ea0-bb7c-f76b4d6dd1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data/results_in_progress_NY_pizza.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if our JSON_FILE already exists. This will prevent us from accidentally overwriting an existing file.\n",
        "\n",
        "If it doesn't exist:\n",
        "\n",
        "* Create any folders needed for the file path.\n",
        "* Save an empty list as JSON_File"
      ],
      "metadata": {
        "id": "IkK7urEm0q5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Check if JSON_FILE exists\n",
        "file_exists = os.path.isfile(JSON_FILE)\n",
        "## If it does not exist:\n",
        "if file_exists == False:\n",
        "\n",
        "    ## CREATE ANY NEEDED FOLDERS\n",
        "    # Get the Folder Name only\n",
        "    folder = os.path.dirname(JSON_FILE)\n",
        "    ## If JSON_FILE included a folder:\n",
        "    if len(folder)>0:\n",
        "        # create the folder\n",
        "        os.makedirs(folder,exist_ok=True)\n",
        "\n",
        "\n",
        "    ## INFORM USER AND SAVE EMPTY LIST\n",
        "    print(f'[i] {JSON_FILE} not found. Saving empty list to file.')\n",
        "\n",
        "\n",
        "    # save an empty list\n",
        "    with open(JSON_FILE,'w') as f:\n",
        "        json.dump([],f)\n",
        "# If it exists, inform user\n",
        "else:\n",
        "    print(f\"[i] {JSON_FILE} already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsRqy7lP0pPd",
        "outputId": "bc0af9d4-3f3d-485b-c549-8ff1ba903b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[i] Data/results_in_progress_NY_pizza.json not found. Saving empty list to file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[i] Data/results_in_progress_NY_pizza.json not found. Saving empty list to a new file.\n",
        "\n",
        "OR\n",
        "\n",
        "[i] Data/results_in_progress_NY_pizza.json already exists.\n",
        "\n",
        "If you get the \"already exists\" message, you already have a file of this name. If you have made this file and have already started making these API calls, you can continue to use this file. If not, you would need to change the name of the file to start with an empty file.\n",
        "\n",
        "\n",
        "**Determine how many results are already in the file**\n",
        "\n",
        "Load the results file to determine the # of results we have previously retrieved. If you just created the file, you would expect it to be empty.\n",
        "\n",
        "We will use this as our offset parameter for our API call. Even if this is your first API call, and the number is 0, we want to define \"n_results\" based on the length of \"previous_results.\""
      ],
      "metadata": {
        "id": "UKotejHU0z10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load previous results and use len of results for offset\n",
        "with open(JSON_FILE,'r') as f:\n",
        "    previous_results = json.load(f)\n",
        "\n",
        "## set offset based on previous results\n",
        "n_results = len(previous_results)\n",
        "print(f'- {n_results} previous results found.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0wnbFKs0wnI",
        "outputId": "3e5eacc9-db6e-4a3b-8494-8df705c5541a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- 0 previous results found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure out how many pages of results we will need**\n",
        "\n",
        "We will perform our first query to get our first page of results and the total number of results. We will then (via code) calculate how many pages we will need to retrieve all of our results."
      ],
      "metadata": {
        "id": "0QGyPtYk1EJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use our yelp_api variable's search_query method to perform our API call\n",
        "results = yelp_api.search_query(location=LOCATION,\n",
        "                                term=TERM,\n",
        "                               offset=n_results)\n",
        "results.keys()"
      ],
      "metadata": {
        "id": "Zsd9HkY51DoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## How many results total?\n",
        "total_results = results['total']\n",
        "total_results"
      ],
      "metadata": {
        "id": "LflxyVYU1AZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## How many did we get the details for?\n",
        "results_per_page = len(results['businesses'])\n",
        "results_per_page"
      ],
      "metadata": {
        "id": "PyuPprZD1Ko9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are over 11000 businesses to retrieve from our API, and we can get 20 results at a time (per \"page\").\n",
        "* We can calculate the # of results remaining by subtracting our offset (length of our previous results) from our total.\n",
        "* Then we can determine how many pages we will need by dividing the results by 20 (or whatever the value happens to be for results per page)\n",
        "* Note that we need to **round u**p the number of pages in order to get all of the results. Even if there is only 1 result on the last page, we want to include that page! To do this, we will use math.ceil."
      ],
      "metadata": {
        "id": "UEczE-2u1M_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use math.ceil to round up for the total number of pages of results.\n",
        "n_pages = math.ceil((results['total']-n_results)/ results_per_page)\n",
        "n_pages"
      ],
      "metadata": {
        "id": "qAP-lwLw1SGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When this example was written, there were 11200 results and 20 results per page. 11200 /20 = 560. But what if we had 11210 results? Rounding UP would give us 561 pages. We expect the first 560 pages to have 20 results and the last page to have the final 10 results. Notice that we have assigned the number of pages as n_pages here. We will use this value in our next segment of code.\n",
        "\n",
        "You can see that having to manually go through 560 pages would be quite time-consuming and inefficient! First, we are going to save the first page into our file, and then we will add to it with a for loop.\n",
        "\n",
        "\n",
        "\n",
        "**Add this page of results to .json file**\n",
        "\n",
        "Our API returns our results in JSON format, with the businesses in a list of dictionaries. We will append the first page of businesses to our previous_results (which is very likely empty) and then save it to disk."
      ],
      "metadata": {
        "id": "TNeBvV8x1YJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join new results with old list with extend and save to file\n",
        "previous_results.extend(results['businesses'])\n",
        "with open(JSON_FILE,'w') as f:\n",
        "     json.dump(previous_results,f)"
      ],
      "metadata": {
        "id": "sWlL-28E1Zlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up a progress bar in our for loop.**\n",
        "\n",
        "To keep us informed about where we are in our loop, we will add a progress bar to our for loop.\n",
        "\n",
        "TQDM is a package designed for adding animated progress bars to Python processes. **It is not currently included in your dojo-env, so you are going to install it manually** by opening a new Terminal/GitBash window and running the following command:  \n",
        "`pip install tqdm`\n",
        "\n",
        "TQDM is easy to use and simply needs to know what we are looping through. If you wanted to test tqdm in action, but your loop is too fast, you can import time and use time.sleep to add a pause within your for loop. We will also use time.sleep when executing many API calls so that we do not overwhelm the server."
      ],
      "metadata": {
        "id": "8-MrMH5Q1ded"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm_notebook\n",
        "import time\n",
        "for i in tqdm_notebook(range(n_pages)):\n",
        "    # adds 200 ms pause\n",
        "    time.sleep(.2)"
      ],
      "metadata": {
        "id": "n3ZQGbgE1iBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Loop to call each page**  \n",
        "\n",
        "The loop below will iterate through each page of the results by starting at the appropriate offset. It will then append the results to the previous_results. This may take some time, so check out the progress bar!"
      ],
      "metadata": {
        "id": "K0YF67BV1zNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm_notebook( range(1,n_pages+1)):\n",
        "\n",
        "    ## Read in results in progress file and check the length\n",
        "    with open(JSON_FILE, 'r') as f:\n",
        "        previous_results = json.load(f)\n",
        "    ## save number of results for to use as offset\n",
        "    n_results = len(previous_results)\n",
        "    ## use n_results as the OFFSET\n",
        "    results = yelp_api.search_query(location=LOCATION,\n",
        "                                    term=TERM,\n",
        "                                    offset=n_results)\n",
        "\n",
        "    ## append new results and save to file\n",
        "    previous_results.extend(results['businesses'])\n",
        "\n",
        "    with open(JSON_FILE,'w') as f:\n",
        "        json.dump(previous_results,f)\n",
        "\n",
        "    # add a 200ms pause\n",
        "    time.sleep(.2)"
      ],
      "metadata": {
        "id": "bguvsq0A1yWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above will return an error. To understand the origin of this error, let's examine the bottom of our error message,\n",
        "\n",
        "YelpAPIError: VALIDATION_ERROR: Too many results requested, limit+offset must be <= 1000.\n",
        "\n",
        "It is telling us that we asked for too many results and that we can only get <= 1,000 results.\n",
        "\n",
        "This is the limitation of using the free tier of Yelp's API. If we were to pay a monthly fee for better access, we would not hit this limitation. Unfortunately, there is no way to adjust our calls to skip those first 1,000. So we can only ever get the same first 1,000 results.\n",
        "\n",
        "So what can we do about it now so that we run our code without error?    \n",
        "\n",
        "## Handling queries with >1000 Results\n",
        "To get around this error, we can add an extra logic check to see if the length of movies we have so far (n_results) + the # of results on each page (results_per_page) is greater than 1,000.\n",
        "\n",
        "If it is greater than 1,000, we will use a break to end our loop early.  \n",
        "\n",
        "**Deleting Our Previous Results FIle**\n",
        "* Let's give ourselves a fresh start with our new and improved loop. Let's delete our previous results file.\n",
        "We could accomplish this manually or programmatically.  \n",
        "\n",
        "**The Manual Way**\n",
        "* We could do this manually by using the Files page in Jupyter to find the file in our results folder and delete it. To delete a file with Jupyter check the check box next to the file you want to delete.\n",
        "\n",
        "* You should now see additional buttons appear above the list of files.\n",
        "\n",
        "  * Click on the red trash can icon to delete the file.\n",
        "\n",
        "  <img src=\"https://assets.codingdojo.com/boomyeah2015/codingdojo/curriculum/content/chapter/1662056603__delete%20with%20jupyter.png\">"
      ],
      "metadata": {
        "id": "43BKQmRk19E2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Programmatic Way**  \n",
        "\n",
        "Just like we used the os module to create new folders, we can also use it to delete (or remove) files.\n",
        "\n",
        "We simply use the os.remove function and pass in our filename to delete.\n",
        "\n",
        "We can then use our os.path.isfile function again to confirm the file no longer exists."
      ],
      "metadata": {
        "id": "nNCmeMZ62j55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## delete file and confirm it no longer exits.\n",
        "os.remove(JSON_FILE)\n",
        "os.path.isfile(JSON_FILE)"
      ],
      "metadata": {
        "id": "N8DnDPgU2GM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now that we've deleted the file, we need to re-run our code to create it.\n",
        "\n",
        "* This process is begging to be turned into a function so we can easily repeat it.\n",
        "\n",
        "* While we are making it a function, we will add the option to delete the JSON file if it already exists, just like we did above.\n",
        "\n",
        "  * So let's make a create_json_file function that accepts the JSON_FILE filename as the first argument and a second argument called delete_if_exists and set it to False by default.\n",
        "\n",
        "  * This way, it will not automatically delete previous search results. We will have to explicitly say delete_if_exists = True to do so."
      ],
      "metadata": {
        "id": "-y6ki0f72pLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_json_file(JSON_FILE,  delete_if_exists=False):\n",
        "\n",
        "    ## Check if JSON_FILE exists\n",
        "    file_exists = os.path.isfile(JSON_FILE)\n",
        "\n",
        "    ## If it DOES exist:\n",
        "    if file_exists == True:\n",
        "\n",
        "        ## Check if user wants to delete if exists\n",
        "        if delete_if_exists==True:\n",
        "\n",
        "            print(f\"[!] {JSON_FILE} already exists. Deleting previous file...\")\n",
        "            ## delete file and confirm it no longer exits.\n",
        "            os.remove(JSON_FILE)\n",
        "            ## Recursive call to function after old file deleted\n",
        "            create_json_file(JSON_FILE,delete_if_exists=False)\n",
        "        else:\n",
        "            print(f\"[i] {JSON_FILE} already exists.\")\n",
        "\n",
        "\n",
        "    ## If it does NOT exist:\n",
        "    else:\n",
        "\n",
        "        ## INFORM USER AND SAVE EMPTY LIST\n",
        "        print(f\"[i] {JSON_FILE} not found. Saving empty list to new file.\")\n",
        "\n",
        "        ## CREATE ANY NEEDED FOLDERS\n",
        "        # Get the Folder Name only\n",
        "        folder = os.path.dirname(JSON_FILE)\n",
        "\n",
        "        ## If JSON_FILE included a folder:\n",
        "        if len(folder)>0:\n",
        "            # create the folder\n",
        "            os.makedirs(folder,exist_ok=True)\n",
        "        ## Save empty list to start the json file\n",
        "        with open(JSON_FILE,'w') as f:\n",
        "            json.dump([],f)"
      ],
      "metadata": {
        "id": "AdAHEcsC2wLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our new function, we can use it with delete_if_exists=True to delete our previous results and start over. We will also need to repeat the steps to recreate our n-results, total_results, results_per_page, and n_pages variables that we created before our first attempted loop."
      ],
      "metadata": {
        "id": "ihKGF1Ko25T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create a new empty json file (exist the previous if it exists)\n",
        "create_json_file(JSON_FILE, delete_if_exists=True)\n",
        "## Load previous results and use len of results for offset\n",
        "with open(JSON_FILE,'r') as f:\n",
        "    previous_results = json.load(f)\n",
        "\n",
        "## set offset based on previous results\n",
        "n_results = len(previous_results)\n",
        "print(f'- {n_results} previous results found.')\n",
        "# use our yelp_api variable's search_query method to perform our API call\n",
        "results = yelp_api.search_query(location=LOCATION,\n",
        "                                term=TERM,\n",
        "                               offset=n_results)\n",
        "## How many results total?\n",
        "total_results = results['total']\n",
        "## How many did we get the details for?\n",
        "results_per_page = len(results['businesses'])\n",
        "# Use math.ceil to round up for the total number of pages of results.\n",
        "n_pages = math.ceil((results['total']-n_results)/ results_per_page)\n",
        "n_pages"
      ],
      "metadata": {
        "id": "flwYZAI2260-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm_notebook( range(1,n_pages+1)):\n",
        "\n",
        "    ## Read in results in progress file and check the length\n",
        "    with open(JSON_FILE, 'r') as f:\n",
        "        previous_results = json.load(f)\n",
        "    ## save number of results for to use as offset\n",
        "    n_results = len(previous_results)\n",
        "\n",
        "    if (n_results + results_per_page) > 1000:\n",
        "        print('Exceeded 1000 api calls. Stopping loop.')\n",
        "        break\n",
        "\n",
        "    ## use n_results as the OFFSET\n",
        "    results = yelp_api.search_query(location=LOCATION,\n",
        "                                    term=TERM,\n",
        "                                    offset=n_results)\n",
        "\n",
        "\n",
        "\n",
        "    ## append new results and save to file\n",
        "    previous_results.extend(results['businesses'])\n",
        "\n",
        "    # display(previous_results)\n",
        "    with open(JSON_FILE,'w') as f:\n",
        "        json.dump(previous_results,f)\n",
        "\n",
        "    time.sleep(.2)"
      ],
      "metadata": {
        "id": "iPhVYX1F29b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After the loop has finished\n",
        "\n",
        "**Convert .json to dataframe**  \n",
        "Load in the \"results in progress\" JSON file into a DataFrame:"
      ],
      "metadata": {
        "id": "FZR2s7893Afu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load final results\n",
        "final_df = pd.read_json(JSON_FILE)\n",
        "display(final_df.head(), final_df.tail())"
      ],
      "metadata": {
        "id": "ymzJ8oJ43F_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for and remove any duplicate results.**"
      ],
      "metadata": {
        "id": "D8jIeWsQ3JkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicate results\n",
        "final_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "7fl0Ei4-3K3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Because our Yelp results include columns that contain lists, we cannot check every column in the dataframe for duplicates.\n",
        "* Instead, we can use the subset argument for df.duplicated() and df.drop_duplicates() to only check the id column for duplicates."
      ],
      "metadata": {
        "id": "hJan0STC3Oel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for duplicate ID's\n",
        "final_df.duplicated(subset='id').sum()"
      ],
      "metadata": {
        "id": "35ETfcgr3R5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Drop duplicate ids and confirm there are no more duplicates\n",
        "final_df = final_df.drop_duplicates(subset='id')\n",
        "final_df.duplicated(subset='id').sum()"
      ],
      "metadata": {
        "id": "4DcserDg3UnA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}